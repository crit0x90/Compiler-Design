{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions in Python: Using the re and PLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we see re and PLY let us check if a string is the same as our \"Barbecue\" string using only standard Python string operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Barbecue = 'Barbecue'\n",
    "s1 = 'Barbecue'\n",
    "\n",
    "Barbecue == s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was pretty easy. Let us now check if a string is the same as our reference in a case-insensitive manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Barbecue = 'Barbecue'\n",
    "s1 = 'barbecue'\n",
    "\n",
    "Barbecue.lower() == s1\n",
    "#Barbecue.upper() == s2.upper() whould have worked as fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a things a little bit more complicated.\n",
    "\n",
    "**Exercise 1.1:**  Write a function that checks if a string is the same as our reference one. The first character is evaluated in a case-insensitive manner but the rest in a case-sensitive one (e.g., Barbecue is the same as barbecue but not as BarbecuE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    s1 = 'Barbecue'\n",
    "    return ((s1[1:] == s[1:]) and (s1[0].lower() == s[0].lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "strings = [\"Barbecue\", \"barbecue\", \"barBecue\", \"Barbecueee\"]\n",
    "for s1 in strings:\n",
    "    print(compare(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get things even more complicated.\n",
    "\n",
    "**Exercise 1.2:** Write a function that checks if a string is the same as our reference.. Once again, the first character is evaluated in a case-insensitive manner and the rest in a case-sensitive one. However, misspells of q as c are disregarded. (e.g., Barbecue is the same as bArbeque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s1):\n",
    "    s = 'Barbecue'\n",
    "    return ((s1[1:].replace('q', 'c') == s[1:].replace('q', 'c')) and (s1[0].lower() == s[0].lower()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "strings = [\"Barbecue\", \"Barbeque\", \"barbecue\", \"bArbequy\"]\n",
    "for s1 in strings:\n",
    "    print(compare(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The re module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re module was added in Python 1.5, and provides Perl-style regular expression patterns. Earlier versions of Python came with the regex module, which provided Emacs-style patterns. The regex module was removed completely in Python 2.5.\n",
    "\n",
    "Regular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as “Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”. You can also use REs to modify a string or to split it apart in various ways.\n",
    "\n",
    "Regular expression patterns are compiled into a series of bytecodes which are then executed by a matching engine written in C. For advanced use, it may be necessary to pay careful attention to how the engine will execute a given RE, and write the RE in a certain way in order to produce bytecode that runs faster. Optimization isn’t covered in this document, because it requires that you have a good understanding of the matching engine’s internals.\n",
    "\n",
    "The regular expression language is relatively small and restricted, so not all possible string processing tasks can be done using regular expressions. There are also tasks that can be done with regular expressions, but the expressions turn out to be very complicated. In these cases, you may be better off writing Python code to do the processing; while Python code will be slower than an elaborate regular expression, it will also probably be more understandable.\n",
    "\n",
    "https://docs.python.org/2/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":\n",
    "\n",
    "|   Syntax metacharacters  | functionality |\n",
    "|------------|\n",
    "|   .  | Matches any character except newline |\n",
    "| ^ | Matches the start of a string |\n",
    "| $ | Matches the end of a string |\n",
    "| *  | 0 or more repetitions |\n",
    "|   +  | 1 or more repetitions|\n",
    "|   ?  | 0 or 1 repetitions|\n",
    "|  {m,n}|  Match from m to n repetitions, attempting to match as many repetitions as possible|\n",
    "|  {m,n}?|  Match from m to n repetitions, attempting to match as few repetitions as possible|\n",
    "| [ ] | Used to indicate a set of characters|\n",
    "| \\ | Escapes special characters or signals a special sequence |\n",
    "| &#124; | or |\n",
    "| ( ) | Matches whatever regular expression is inside the parentheses |\n",
    "\n",
    "Compile a regular expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The match method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_print(RE, s1, flag=0):\n",
    "    Obj = re.match(RE, s1, flag)\n",
    "    if Obj:\n",
    "        return \"Success, the matched string is {}\".format(Obj.group())\n",
    "    else:\n",
    "        return \"Failed to find a match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if a string is the same as our \"Barbecue\" reference string using re:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = 'Barbecue'\n",
    "s1 = 'Barbecue'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if a string is the same as our \"Barbecue\" reference in a case insensitive manner using re:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = '(B|b)(A|a)(R|r)(B|b)(E|e)(C|c)(U|u)(E|e)'\n",
    "s1 = 'barbecue'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a compilation flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, the matched string is barbecue\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "RE = 'Barbecue'\n",
    "s1 = 'barbecue'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1, re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us revisit Exercise 1.1. The first character is evaluated in a case-insensitive manner but the rest in a case-sensitive one (e.g., Barbecue is the same as barbecue but not as BarbecuE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = '(B|b)arbecue'\n",
    "s1 = 'barbecue'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lett's revisit Exercise 1.2 and try to implement it using REs\n",
    "\n",
    "**Exercise 1.3:** Write a routine that checks if a string is the same as our reference. Once again, the first character is evaluated in a case-insensitive manner and the rest in a case-sensitive one. However, misspells of q as c are disregarded. (e.g., Barbecue is the same as bArbeque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, the matched string is barbeque\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RE = '(B|b)arbe(c|q)ue'\n",
    "s1 = 'barbeque'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to match varying sets of characters is the first thing regular expressions can do that isnt already possible with the methods available on strings. However, if that was the only additional capability of res, they wouldn't be much of an advance. Another capability is that you can specify that portions of the RE must be repeated a certain number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, the matched string is aaa\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RE = 'ab*aa'\n",
    "s1 = 'aaa'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, the matched string is abbbbbbbbaa\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RE = 'ab*aa'\n",
    "s1 = 'abbbbbbbbaa'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The match vs the search method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match finds something at the beginning of the string and returns a match object.\n",
    "\n",
    "What will happen in the following?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def compare_print(RE, s1, flag=0):\n",
    "    Obj = re.match(RE, s1, flag)\n",
    "    if Obj:\n",
    "        return \"Success, the matched string is {}\".format(Obj.group())\n",
    "    else:\n",
    "        return \"Failed to find a match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find a match\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "RE = 'aa(b)*aa'\n",
    "s1 = 'bbaaaa'\n",
    "\n",
    "\n",
    "print(match_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search method finds something anywhere in the string and return a match object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def search_print(RE, s1, flag=0):\n",
    "    Obj = re.search(RE, s1, flag)\n",
    "    if Obj:\n",
    "        return \"Success, the matched string is {}\".format(Obj.group())\n",
    "    else:\n",
    "        return \"Failed to find a match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the following code produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, the matched string is aaaa\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "RE = 'aa(b)*aa'\n",
    "s1 = 'bbaaaa'\n",
    "\n",
    "\n",
    "print(search_print(RE, s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these methods return all this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match and search methods return a match and a search object, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that \\w is a special character and any alphanumeric character and the underscore;\n",
    "#this is equivalent to the set [a-zA-Z0-9_]\n",
    "\n",
    "RE= \"(\\w+) (\\w+)\"\n",
    "s1 = \"Isaac Newton, physicist\"\n",
    "\n",
    "Obj = re.match(RE, s1)\n",
    "Obj = re.search(RE, s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the print methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_print(RE, s1, flag=0):\n",
    "    # The call is happening here\n",
    "    Obj = re.match(RE, s1, flag)\n",
    "    if Obj: \n",
    "        return \"Success, the matched string is {}\".format(Obj.group()) #Both Objects have a group() method\n",
    "    else:\n",
    "        return \"Failed to find a match\"\n",
    "    \n",
    "def search_print(RE, s1, flag=0):\n",
    "    # The call is happening here\n",
    "    Obj = re.search(RE, s1, flag)\n",
    "    if Obj:\n",
    "        return \"Success, the matched string is {}\".format(Obj.group()) #Both Objects have a group() method\n",
    "    else:\n",
    "        return \"Failed to find a match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Objects have a group() method. The groups act as a classification scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE= \"(\\w+) (\\w+)\"\n",
    "s1 = \"Isaac Newton, physicist\"\n",
    "Obj = re.match(RE, s1)\n",
    "\n",
    "print(Obj.group(0))     # The entire match (or m.group(0))\n",
    "print(Obj.group(1))     # The first parenthesized subgroup.\n",
    "print(Obj.group(2))     # The second parenthesized subgroup.\n",
    "print(Obj.group(1, 2))  # Multiple arguments give us a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Objects have a start() and end() method that give the beginning and end of the found string, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12\n",
      "0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "RE= \"(\\w+) (\\w+)\"\n",
    "s1 = \"Isaac Newton, physicist\"\n",
    "\n",
    "Obj = re.match(RE, s1)\n",
    "print(Obj.start())    \n",
    "print(Obj.end())\n",
    "\n",
    "Obj = re.search(RE, s1)\n",
    "print(Obj.start())    \n",
    "print(Obj.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4:**\n",
    "\n",
    "Write a routine that will remove \"kill_avengers\" from my email address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"apanagopoulos@mail.frekill_avengerssnostate.edu\"\n",
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "import re\n",
    "\n",
    "def search_print(RE, s1, flag=0):\n",
    "    # The call is happening here\n",
    "    Obj = re.search(RE, s1, flag)\n",
    "    if Obj:\n",
    "        return \"Success, the matched string is {}\".format(Obj.group()) #Both Objects have a group() method\n",
    "    else:\n",
    "        return \"Failed to find a match\"\n",
    "\n",
    "def removeKA(email):\n",
    "    print(email.replace(\"kill_avengers\", \"\"))\n",
    "    \n",
    "def removeKAwithRE(email):\n",
    "    Obj = re.search(\"kill_avengers\", email)\n",
    "    email[:obj.start()] + email[:obj.end()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.5:** Write a routine that finds all unique 0-9 digits present in the following string:\n",
    "\n",
    "\"a1afs4236hghshj34jsdjdkdgh3hkdk4jdk4kjhdjdkklsdneuiewui3jhnsnad2323434nasdjkn223jjklasdjkl\"\n",
    "\n",
    "(Hint: You can use the re.finditer() iterator and lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"a1afs4236hghshj34jsdjdkdgh3hkdk4jdk4kjhdjdkklsdneuiewui3jhnsnad2323434nasdjkn223jjklasdjkl\"\n",
    "RE = '[1-9]'\n",
    "# WRITE YOUR CODE HERE\n",
    "{int(Obj.group()) for Obj in re.finditer(RE, s1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baa found\n"
     ]
    }
   ],
   "source": [
    "RE = 'baa'\n",
    "s1 = 'baaaa'\n",
    "\n",
    "#precomiling the regular expression\n",
    "automaton = re.compile(RE)\n",
    "result = automaton.match(s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baa found\n"
     ]
    }
   ],
   "source": [
    "RE = 'baa'\n",
    "s1 = 'baaaa'\n",
    "#compiling the RE dynamically\n",
    "result = re.match(RE, s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it speeds up things if a regular expression is used regularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9980368614196777\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "automaton = re.compile(RE)\n",
    "for i in range(0,5000000):\n",
    "    result = automaton.match(s1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.559620380401611\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(0,5000000):\n",
    "    result = re.match(RE, s1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Backslash Plague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+*{ok} found\n"
     ]
    }
   ],
   "source": [
    "RE = '\\+\\*{.*}'\n",
    "s1 = '+*{ok}'\n",
    "\n",
    "result = re.match(RE, s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.6:** \n",
    "Write a regular expression that recognizes the string: \\What is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\What is going on? found\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR RE HERE\n",
    "s1 = \"\\What is going on?\"\n",
    "#have to escape twice because it is also a special character in Python (not just in the RE library)\n",
    "RE = '\\\\\\\\What is going on\\?'\n",
    "result = re.match(RE, s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you want to write a RE that matches the string \\\\section, which might be found in a LaTeX file. To figure out what to write in the program code, start with the desired string to be matched. Next, you must escape any backslashes and other metacharacters by preceding them with a backslash, resulting in the string \\\\\\\\section. The resulting string that must be passed to re.compile() must be \\\\\\\\section. However, to express this as a Python string literal, both backslashes must be escaped again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\What is going on?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.7:** \n",
    "Write a regular expression that recognizes the string: \\\\\\\\When is this going to end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\When is this going to end? found\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR RE HERE\n",
    "s1 = \"\\\\\\\\When is this going to end?\"\n",
    "RE = \"\\\\\\\\\\\\\\\\When is this going to end\\?\"\n",
    "result = re.match(RE, s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use raw input in Python to make things a bit clearer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\When is this going to end? found\n"
     ]
    }
   ],
   "source": [
    "RE = r\"\\\\\\\\When is this going to end\\?\"\n",
    "s1 = r\"\\\\When is this going to end?\"\n",
    "\n",
    "result = re.match(RE, s1)\n",
    "\n",
    "print(\"{} found\".format(result.group()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PLY for lexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLY is a pure-Python implementation of the popular compiler construction tools lex and yacc. The main goal of PLY is to stay fairly faithful to the way in which traditional lex/yacc tools work. This includes supporting LALR(1) parsing as well as providing extensive input validation, error reporting, and diagnostics. Thus, if you've used yacc in another programming language, it should be relatively straightforward to use PLY.\n",
    "Early versions of PLY were developed to support an Introduction to Compilers Course I taught in 2001 at the University of Chicago. Since PLY was primarily developed as an instructional tool, you will find it to be fairly picky about token and grammar rule specification. In part, this added formality is meant to catch common programming mistakes made by novice users. However, advanced users will also find such features to be useful when building complicated grammars for real programming languages. It should also be noted that PLY does not provide much in the way of bells and whistles (e.g., automatic construction of abstract syntax trees, tree traversal, etc.). Nor would I consider it to be a parsing framework. Instead, you will find a bare-bones, yet fully capable lex/yacc implementation written entirely in Python.\n",
    "\n",
    "The rest of this document assumes that you are somewhat familiar with parsing theory, syntax directed translation, and the use of compiler construction tools such as lex and yacc in other programming languages. If you are unfamiliar with these topics, you will probably want to consult an introductory text such as \"Compilers: Principles, Techniques, and Tools\", by Aho, Sethi, and Ullman. O'Reilly's \"Lex and Yacc\" by John Levine may also be handy. In fact, the O'Reilly book can be used as a reference for PLY as the concepts are virtually identical.\n",
    "\n",
    "http://www.dabeaz.com/ply/ply.html\n",
    "\n",
    "* python3 –m pip install --upgrade pip setuptools wheel\n",
    "* sudo pip3 install ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No t_error rule is defined\n"
     ]
    }
   ],
   "source": [
    "import ply.lex as lex\n",
    "\n",
    "## Define the tokens\n",
    "#order of definition matters\n",
    "tokens = ('Class1', 'Class2')\n",
    "t_Class1 = 'aab' #class names must match the tokens\n",
    "t_Class2= 'dvd'\n",
    "\n",
    "## Create the lexer\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us include a panic mode recovery error function as function token, we will SE function tokens again below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n",
    "%reset -f\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ('Class1', 'Class2')\n",
    "\n",
    "t_Class1 = 'aab'\n",
    "t_Class2= 'dvd'\n",
    "\n",
    "def t_error( t ):\n",
    "  print(\"Invalid Token:\",t.value[0])\n",
    "  t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the tokens are requested by the parser but for debugging purposes we can include a procedure that tokenizes an input string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1 : aab 0\n",
      "Invalid Token: a\n",
      "Invalid Token: a\n"
     ]
    }
   ],
   "source": [
    "lex.input(\"aabaa\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value, tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest matching rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n",
    "%reset -f\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class2 : aabaa 0\n"
     ]
    }
   ],
   "source": [
    "tokens = ('Class1', 'Class2')\n",
    "\n",
    "t_Class1 = 'aab'\n",
    "t_Class2= 'aabaa'\n",
    "\n",
    "def t_error( t ):\n",
    "  print(\"Invalid Token:\",t.value[0])\n",
    "  t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer\n",
    "\n",
    "lex.input(\"aabaa\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value, tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest matching rule implication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class2 : aabaa 0\n",
      "Invalid Token: b\n"
     ]
    }
   ],
   "source": [
    "lex.input(t_Class1+t_Class1)\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value, tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n",
    "%reset -f\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class2 : aabaa , 0\n",
      "Invalid Token:  \n",
      "Class2 : aabaa , 6\n",
      "Invalid Token:  \n",
      "Invalid Token: \n",
      "\n",
      "Invalid Token:  \n",
      "Class2 : aabaa , 14\n",
      "Invalid Token:  \n",
      "Invalid Token: x\n",
      "Invalid Token: \t\n",
      "Invalid Token: x\n",
      "Invalid Token:  \n",
      "Class2 : aabaa , 24\n",
      "Invalid Token:  \n",
      "Class3 : 10 , 30\n"
     ]
    }
   ],
   "source": [
    "tokens = ('Class1', 'Class2', 'Class3')\n",
    "\n",
    "t_Class1 = 'aab'\n",
    "t_Class2= 'aabaa'\n",
    "\n",
    "def t_Class3(t):\n",
    "    r'cd+'\n",
    "    t.value = 10\n",
    "    return t\n",
    "\n",
    "def t_error( t ):\n",
    "  print(\"Invalid Token:\",t.value[0])\n",
    "  t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer\n",
    "\n",
    "  \n",
    "lex.input(\"aabaa aabaa \\n aabaa x\\tx aabaa cddddd\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value,\",\",tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip spaces and tabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n",
    "%reset -f\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class2 : aabaa , 0\n",
      "Class2 : aabaa , 6\n",
      "Invalid Token: \n",
      "\n",
      "Class2 : aabaa , 14\n",
      "Class2 : aabaa , 22\n",
      "Class3 : 10 , 28\n"
     ]
    }
   ],
   "source": [
    "tokens = ('Class1', 'Class2', 'Class3')\n",
    "\n",
    "t_ignore = ' \\t'\n",
    "\n",
    "t_Class1 = 'aab'\n",
    "t_Class2= 'aabaa'\n",
    "\n",
    "def t_Class3(t):\n",
    "    r'cd+'\n",
    "    t.value = 10\n",
    "    return t\n",
    "\n",
    "def t_error( t ):\n",
    "  print(\"Invalid Token:\",t.value[0])\n",
    "  t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer\n",
    "\n",
    "  \n",
    "lex.input(\"aabaa aabaa \\n aabaa \\t aabaa cddddd\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value,\",\",tok.lexpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip new line but also count line number in a new token argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n",
    "%reset -f\n",
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class2 : aabaa , 0 , 1\n",
      "Class2 : aabaa , 6 , 1\n",
      "Class2 : aabaa , 14 , 2\n",
      "Class2 : aabaa , 22 , 2\n",
      "Class3 : 10 , 28 , 2\n"
     ]
    }
   ],
   "source": [
    "tokens = ('Class1', 'Class2', 'Class3')\n",
    "\n",
    "t_ignore = ' \\t'\n",
    "\n",
    "t_Class1 = 'aab'\n",
    "t_Class2= 'aabaa'\n",
    "\n",
    "def t_Class3(t):\n",
    "    r'cd+'\n",
    "    t.value = 10\n",
    "    return t\n",
    "\n",
    "def t_newline( t ):\n",
    "  r'\\n+'\n",
    "  t.lexer.lineno += len( t.value )\n",
    "\n",
    "def t_error( t ):\n",
    "  print(\"Invalid Token:\",t.value[0])\n",
    "  t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex() # lexer object holds the ccreated lexer\n",
    "\n",
    "  \n",
    "lex.input(\"aabaa aabaa \\n aabaa \\t aabaa cddddd\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value,\",\",tok.lexpos,\",\",tok.lineno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.8:** \n",
    "Write a simple calculator lexer that lexes mathematical expressions.\n",
    "\n",
    "The calculator lexer should be able to recognize all whole number digits [0-inf) (starting with 0s is fine but should store their integer value), the operators +,-,=,*,\\ and the () punctuation to assign priorities. The lexer should also be able to tokennize variable name tokens. The name tokens can only include english characters and the underscore (e.g., s, asd. Xok, k_pao) but cannot begin or end with an underscore. The lexer should return the type of the token, its value, position and line number. The lexer should skip all white spaces and tabs except new lines which should be tokenized with the value -1.\n",
    "\n",
    "**Do not forget to run the reset code before every lexer generation**\n",
    "\n",
    "*Self asses your implementation:*\n",
    "\n",
    "* lex.input(\"Old_X \\t= 0 \\n New_X \\t= 4 + 0126 - (Old_X+3)\")\n",
    "\n",
    "Shlould yield:\n",
    "\n",
    "NAME : Old_X , 0 , 1<br/>\n",
    "EQUAL : = , 7 , 1<br/>\n",
    "NUMBER : 0 , 9 , 1<br/>\n",
    "NEWLINE : -1 , 11 , 1<br/>\n",
    "NAME : New_X , 13 , 2<br/>\n",
    "EQUAL : = , 20 , 2<br/>\n",
    "NUMBER : 4 , 22 , 2<br/>\n",
    "PLUS : + , 24 , 2<br/>\n",
    "NUMBER : 126 , 26 , 2<br/>\n",
    "MINUS : - , 31 , 2<br/>\n",
    "LPAREN : ( , 33 , 2<br/>\n",
    "NAME : Old_X , 34 , 2<br/>\n",
    "PLUS : + , 39 , 2<br/>\n",
    "NUMBER : 3 , 40 , 2<br/>\n",
    "RPAREN : ) , 41 , 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to avoid conflicts and re-import ply.lex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-3bc31a13293b>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-3bc31a13293b>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    global current_pos = 0\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "%reset -f\n",
    "import ply.lex as lex\n",
    "\n",
    "tokens = (\"Name\",\"Equal\",\"Leftpar\", \"Rightpar\", \"Plus\", \"Minus\", \"Mul\", \"Div\", \"Num\")\n",
    "global current_pos = 0\n",
    "\n",
    "t_ignore = ' \\t'\n",
    "\n",
    "def t_newline( t ):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len( t.value )\n",
    "    current_pos = 0\n",
    "    \n",
    "def t_Name( t ):\n",
    "    r'[a-zA-Z][a-zA-Z_]*[a-zA-Z]|[a-zA-Z]'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "    \n",
    "def t_error( t ):\n",
    "    print(\"Invalid Token:\",t.value[0])\n",
    "    t.lexer.skip(1) #Panic mode recovery. Skip one character\n",
    "    \n",
    "def t_Equal( t ):\n",
    "    r'\\='\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Leftpar( t ):\n",
    "    r'\\('\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Rightpar( t ):\n",
    "    r'\\)'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Plus( t ):\n",
    "    r'\\+'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Minus( t ):\n",
    "    r'\\-'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Mul( t ):\n",
    "    r'\\*'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Div( t ):\n",
    "    r'\\/'\n",
    "    current_pos += len(t.value)\n",
    "    return t\n",
    "\n",
    "def t_Num( t ):\n",
    "    r'[0-9]+'\n",
    "    current_pos += len(t.value)\n",
    "    t.value = str(int(t.value)) \n",
    "    return t\n",
    "\n",
    "\n",
    "__file__ = \"Untitled.ipynb\" #FIX to work with Jupyter\n",
    "lexer = lex.lex()\n",
    "\n",
    "#lex.input(\"Old_X =\")\n",
    "lex.input(\"Old_X \\t= 0 \\n New_X \\t= 4 + 0126 - (Old_X+3)\")\n",
    "for tok in iter(lex.token, None):\n",
    "    print(tok.type ,\":\", tok.value,\",\",tok.lexpos,\",\",tok.lineno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
